{
    "cells": [
        {
            "cell_type": "markdown",
            "source": [
                "# Rice Leaf Diseases: Implementing a Custom Model - Part 2\n",
                "## Using Transfer Learning to \"warm up\" our model's weights.\n",
                "\n",
                "This notebook will continue our exploration of the [Rice Leaf Diseases dataset from Kaggle](https://www.kaggle.com/vbookshelf/rice-leaf-diseases).  \n",
                "\n",
                "In this notebook, we will do the following:\n",
                "\n",
                "1. Create a CNN-based classifier network similar to [1], as we did in the previous notebook.\n",
                "2. Load pre-trained weights for transfer learning.  The model was pre-trained on the \"Plant Leaves\" dataset available at [2].\n",
                "3. Specialize the network on our rice image dataset (training partition), using lots of image augmentation to prevent overfitting.\n",
                "4. Predict on the testing partition, and display results with a confusion matrix and classification report.\n",
                "\n",
                "[1]: Wick, C. & Puppe, F. Leaf Identification Using a Deep Convolutional Neural Network. arXiv:1712.00967 [cs] (2017).\n",
                "\n",
                "[2]: CHOUHAN, Siddharth Singh; Kaul, Ajay; SINGH, UDAY PRATAP; Madhav Institute of Technology & Science  (2019), “A Database of Leaf Images: Practice towards Plant Conservation with Plant Pathology”, Mendeley Data, V1, doi: 10.17632/hb74ynkjcn.1 <https://data.mendeley.com/datasets/hb74ynkjcn/1>\n",
                "\n",
                "## Setting up your environment\n",
                "Before running this notebook, you will need to make sure you have [downloaded](https://www.kaggle.com/vbookshelf/rice-leaf-diseases) the dataset and extracted the files.  This notebook assumes the image data is extracted in the same directory as this notebook, and that the top-level data directory is named \"rice_leaf_diseases\".  You can edit the code if those assumptions do not hold on your own setup.\n",
                "\n",
                "## Python environment\n",
                "You will need the following packages installed to execute the code shown in this notebook:\n",
                "\n",
                "* [Matplotlib](https://matplotlib.org/)\n",
                "* [Numpy](https://numpy.org/)\n",
                "* [Pandas](https://pandas.pydata.org/)\n",
                "* [Scikit-Learn](https://scikit-learn.org/)\n",
                "* [Tensorflow](https://www.tensorflow.org/)\n",
                "\n",
                "## Local directory structure\n",
                "This notebook assumes you have the rice leaf images in the following directory structure:\n",
                "\n",
                "    rice_leaf_diseases/\n",
                "        Bacterial leaf blight/\n",
                "        Brown spot/\n",
                "        Leaf smut/\n",
                "\n",
                "The \"ground truth\" file \"_rice_leaf_diseases_ground_truth.csv_\" was created in the previous (\"rice_image_EDA\") notebook.  If you don't have this file, you can create it by running the first several cells in that notebook, which is available [by clicking here](https://gist.github.com/jcausey-astate/207ba4d65126abe0482b740b41117f9e).\n",
                "\n",
                "The pre-trained weights we will be using are available at: <TODO-LINK-HERE>"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "from sklearn.model_selection import StratifiedShuffleSplit\n",
                "import sklearn.metrics as metrics\n",
                "import tensorflow as tf\n",
                "import tensorflow.keras as keras\n",
                "import matplotlib.pyplot as plt"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "First, we will create a function that will actually define the structure of the network.  We use Wick et al. (2017) as a guide, and fill in the missing details with common choices (activations, amount of dropout, etc.)."
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "source": [
                "def build_leaf_classifier_model(input_shape=(256,256,3), n_classes=3):\n",
                "    '''\n",
                "    This function builds a Keras CNN similar to the one described in Wick et al. (2017).\n",
                "    '''\n",
                "    lrelu = keras.layers.LeakyReLU()\n",
                "    input = keras.layers.Input(shape=input_shape)\n",
                "    x = keras.layers.Conv2D(40, (3,3), activation=lrelu, padding='same', name=\"conv1\")(input)\n",
                "    x = keras.layers.MaxPool2D((2,2), name='max_pool1')(x)\n",
                "    x = keras.layers.Conv2D(40, (4,4), activation=lrelu, padding='same', name='conv2')(x)\n",
                "    x = keras.layers.MaxPool2D((2,2), name='max_pool2')(x)\n",
                "    x = keras.layers.Conv2D(80, (4,4), activation=lrelu, padding='same', name='conv3')(x)\n",
                "    x = keras.layers.MaxPool2D((2,2), name='max_pool3')(x)\n",
                "    x = keras.layers.Conv2D(160, (4,4), activation=lrelu, padding='same', name='conv4')(x)\n",
                "    x = keras.layers.MaxPool2D((2,2), name='max_pool4')(x)\n",
                "    x = keras.layers.Flatten(name='flatten')(x)\n",
                "    x = keras.layers.Dense(500, activation=lrelu, name='dense_feature')(x)\n",
                "    x = keras.layers.Dropout(0.20, name='dropout1')(x)\n",
                "    x = keras.layers.Dense(n_classes, activation='softmax', name='dense_output')(x)\n",
                "    return keras.Model(inputs=input, outputs=x)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "Let's create an instance of the model and see the summary:"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "source": [
                "plant_leaves_model = build_leaf_classifier_model(n_classes=22) # Need 22 classes for pre-trained model\n",
                "plant_leaves_model.summary()"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Model: \"model\"\n",
                        "_________________________________________________________________\n",
                        "Layer (type)                 Output Shape              Param #   \n",
                        "=================================================================\n",
                        "input_1 (InputLayer)         [(None, 256, 256, 3)]     0         \n",
                        "_________________________________________________________________\n",
                        "conv1 (Conv2D)               (None, 256, 256, 40)      1120      \n",
                        "_________________________________________________________________\n",
                        "max_pool1 (MaxPooling2D)     (None, 128, 128, 40)      0         \n",
                        "_________________________________________________________________\n",
                        "conv2 (Conv2D)               (None, 128, 128, 40)      25640     \n",
                        "_________________________________________________________________\n",
                        "max_pool2 (MaxPooling2D)     (None, 64, 64, 40)        0         \n",
                        "_________________________________________________________________\n",
                        "conv3 (Conv2D)               (None, 64, 64, 80)        51280     \n",
                        "_________________________________________________________________\n",
                        "max_pool3 (MaxPooling2D)     (None, 32, 32, 80)        0         \n",
                        "_________________________________________________________________\n",
                        "conv4 (Conv2D)               (None, 32, 32, 160)       204960    \n",
                        "_________________________________________________________________\n",
                        "max_pool4 (MaxPooling2D)     (None, 16, 16, 160)       0         \n",
                        "_________________________________________________________________\n",
                        "flatten (Flatten)            (None, 40960)             0         \n",
                        "_________________________________________________________________\n",
                        "dense_feature (Dense)        (None, 500)               20480500  \n",
                        "_________________________________________________________________\n",
                        "dropout1 (Dropout)           (None, 500)               0         \n",
                        "_________________________________________________________________\n",
                        "dense_output (Dense)         (None, 22)                11022     \n",
                        "=================================================================\n",
                        "Total params: 20,774,522\n",
                        "Trainable params: 20,774,522\n",
                        "Non-trainable params: 0\n",
                        "_________________________________________________________________\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Load the pre-trained weights from the Plant Leaves dataset:"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "source": [
                "plant_leaves_weights = \"plant_leaves_weights_30-0.35.h5\"\n",
                "plant_leaves_model.load_weights(plant_leaves_weights)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Customize the model for our rice data:\n",
                "We need to replace the classifier layer, creating a rice leaf model.  We will add one hidden layer (a Dense layer with 64 units) and an output layer to give us probabilities for our 3 classes.  These will receive the output from the feature layer in the pre-trained model (which has 500 units)."
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "source": [
                "# Replace the classifier layer, creating a rice leaf model:\n",
                "plant_feature_layer = plant_leaves_model.get_layer(\"dense_feature\").output\n",
                "new_features_layer = keras.layers.Dense(64, activation='relu', name=\"rice_features\")(plant_feature_layer)\n",
                "new_output_layer = keras.layers.Dense(3, activation='softmax', name=\"rice_outputs\")(new_features_layer)\n",
                "rice_disease_model = keras.Model(\n",
                "    inputs=plant_leaves_model.input,\n",
                "    outputs=new_output_layer\n",
                ")"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "source": [
                "rice_disease_model.summary()"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Model: \"model_1\"\n",
                        "_________________________________________________________________\n",
                        "Layer (type)                 Output Shape              Param #   \n",
                        "=================================================================\n",
                        "input_1 (InputLayer)         [(None, 256, 256, 3)]     0         \n",
                        "_________________________________________________________________\n",
                        "conv1 (Conv2D)               (None, 256, 256, 40)      1120      \n",
                        "_________________________________________________________________\n",
                        "max_pool1 (MaxPooling2D)     (None, 128, 128, 40)      0         \n",
                        "_________________________________________________________________\n",
                        "conv2 (Conv2D)               (None, 128, 128, 40)      25640     \n",
                        "_________________________________________________________________\n",
                        "max_pool2 (MaxPooling2D)     (None, 64, 64, 40)        0         \n",
                        "_________________________________________________________________\n",
                        "conv3 (Conv2D)               (None, 64, 64, 80)        51280     \n",
                        "_________________________________________________________________\n",
                        "max_pool3 (MaxPooling2D)     (None, 32, 32, 80)        0         \n",
                        "_________________________________________________________________\n",
                        "conv4 (Conv2D)               (None, 32, 32, 160)       204960    \n",
                        "_________________________________________________________________\n",
                        "max_pool4 (MaxPooling2D)     (None, 16, 16, 160)       0         \n",
                        "_________________________________________________________________\n",
                        "flatten (Flatten)            (None, 40960)             0         \n",
                        "_________________________________________________________________\n",
                        "dense_feature (Dense)        (None, 500)               20480500  \n",
                        "_________________________________________________________________\n",
                        "rice_features (Dense)        (None, 64)                32064     \n",
                        "_________________________________________________________________\n",
                        "rice_outputs (Dense)         (None, 3)                 195       \n",
                        "=================================================================\n",
                        "Total params: 20,795,759\n",
                        "Trainable params: 20,795,759\n",
                        "Non-trainable params: 0\n",
                        "_________________________________________________________________\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "This model has almost 21 million parameters, but we have already trained most of them (all except the $32,064 + 195 = 32,259$ parameters from the new feature and output layers).  \n",
                "\n",
                "We will fit those ~32K new weights on a training dataset, with augmentation.\n",
                "\n",
                "Let's load the dataset and use `StratifiedShuffleSplit` to create an 80/20 train/test split:"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "source": [
                "ground_truth_df = pd.read_csv(\"rice_leaf_diseases_ground_truth.csv\")\n",
                "ground_truth_df.head()\n",
                "sss = StratifiedShuffleSplit(1, test_size=0.20, random_state=2021)\n",
                "train_indices, test_indices = list(sss.split(ground_truth_df.values, ground_truth_df['class'].values))[0]\n",
                "print(f\"Training set has {train_indices.shape[0]} samples.\")\n",
                "print(f\"Test set has {test_indices.shape[0]} samples.\")"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Training set has 96 samples.\n",
                        "Test set has 24 samples.\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "Now we will set up our image data generators for training and testing.\n",
                "\n",
                "The training generator will not do any augmentation.  It will only scale the image to contain values in the $[0,1]$ numeric range.\n",
                "\n",
                "The test generator will perform several kinds of augmentation, in addition to rescaling the image to $[0,1]$: \n",
                "\n"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "source": [
                "# Declare the generator (we will take default initial paramters for now)\n",
                "test_generator = keras.preprocessing.image.ImageDataGenerator(\n",
                "    rescale=1.0/255.0\n",
                ")\n",
                "train_generator = keras.preprocessing.image.ImageDataGenerator(\n",
                "    rescale=1.0/255.0,\n",
                "    horizontal_flip=True,\n",
                "    vertical_flip=True,\n",
                "    rotation_range=60,\n",
                "    width_shift_range=0.20,\n",
                "    height_shift_range=0.10,\n",
                "    zoom_range=0.2,\n",
                "    brightness_range=(0.5, 1.25),\n",
                ")"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "Now, we will create a \"flow\" (this is actually the seqeunce of data that will feed the model).  We want one for training that uses only the \"train\" rows in the ground truth dataframe, and one for testing that uses only the \"test\" rows.  The training flow will be shuffled, the testing one need not be, so we won't."
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "source": [
                "BATCH_SIZE = 32\n",
                "train_dataflow = train_generator.flow_from_dataframe(\n",
                "    ground_truth_df.iloc[train_indices,:],\n",
                "    x_col = \"image_path\",\n",
                "    y_col = \"class\",\n",
                "    target_size = (256,256),\n",
                "    shuffle = True,\n",
                "    seed = 2021,\n",
                "    batch_size=BATCH_SIZE,\n",
                ")\n",
                "test_dataflow = test_generator.flow_from_dataframe(\n",
                "    ground_truth_df.iloc[test_indices,:],\n",
                "    x_col = \"image_path\",\n",
                "    y_col = \"class\",\n",
                "    target_size = (256,256),\n",
                "    shuffle = False,\n",
                "    seed = 2021,\n",
                "    batch_size=BATCH_SIZE,\n",
                ")"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Found 96 validated image filenames belonging to 3 classes.\n",
                        "Found 24 validated image filenames belonging to 3 classes.\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "Now, we can compile the model and train it.  But we need to do a little work to make sure we are only training the _new_ layers, not changing the training on the pre-trained layers:\n",
                "\n",
                "1. Set the whole model to \"not trainable\" - this gets all layers in one line.\n",
                "2. Set our new \"feature\" layer (`rice_features`) to \"trainable\".\n",
                "3. Set our output layer (`rice_outputs`) to \"trainable\".\n",
                "\n",
                "Then we can use the `fit()` method to train the model for a while.  We use 30 epochs here."
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "source": [
                "rice_disease_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n",
                "rice_disease_model.trainable = False\n",
                "rice_disease_model.get_layer(\"rice_features\").trainable = True\n",
                "rice_disease_model.get_layer(\"rice_outputs\").trainable = True\n",
                "\n",
                "rice_disease_model.fit(\n",
                "    train_dataflow,\n",
                "    validation_data=test_dataflow,\n",
                "    epochs=30,\n",
                "    batch_size=BATCH_SIZE\n",
                ")"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Epoch 1/30\n",
                        "3/3 [==============================] - 7s 2s/step - loss: 4.5445 - acc: 0.3021 - val_loss: 4.1712 - val_acc: 0.3333\n",
                        "Epoch 2/30\n",
                        "3/3 [==============================] - 7s 2s/step - loss: 1.9246 - acc: 0.4479 - val_loss: 1.5081 - val_acc: 0.3750\n",
                        "Epoch 3/30\n",
                        "3/3 [==============================] - 6s 2s/step - loss: 1.3108 - acc: 0.4479 - val_loss: 0.7804 - val_acc: 0.7500\n",
                        "Epoch 4/30\n",
                        "3/3 [==============================] - 6s 2s/step - loss: 0.8157 - acc: 0.6042 - val_loss: 0.9802 - val_acc: 0.6667\n",
                        "Epoch 5/30\n",
                        "3/3 [==============================] - 6s 2s/step - loss: 0.9428 - acc: 0.6979 - val_loss: 0.7191 - val_acc: 0.7500\n",
                        "Epoch 6/30\n",
                        "3/3 [==============================] - 6s 2s/step - loss: 0.5865 - acc: 0.7917 - val_loss: 0.5919 - val_acc: 0.7083\n",
                        "Epoch 7/30\n",
                        "3/3 [==============================] - 6s 2s/step - loss: 0.5769 - acc: 0.8021 - val_loss: 0.4242 - val_acc: 0.8750\n",
                        "Epoch 8/30\n",
                        "3/3 [==============================] - 6s 2s/step - loss: 0.5714 - acc: 0.7917 - val_loss: 0.6613 - val_acc: 0.7917\n",
                        "Epoch 9/30\n",
                        "3/3 [==============================] - 7s 2s/step - loss: 0.5565 - acc: 0.7812 - val_loss: 0.2958 - val_acc: 0.9583\n",
                        "Epoch 10/30\n",
                        "3/3 [==============================] - 7s 2s/step - loss: 0.5291 - acc: 0.7604 - val_loss: 0.2867 - val_acc: 0.8750\n",
                        "Epoch 11/30\n",
                        "3/3 [==============================] - 7s 2s/step - loss: 0.4990 - acc: 0.7812 - val_loss: 0.5545 - val_acc: 0.7083\n",
                        "Epoch 12/30\n",
                        "3/3 [==============================] - 7s 2s/step - loss: 0.5684 - acc: 0.7812 - val_loss: 0.2100 - val_acc: 0.8750\n",
                        "Epoch 13/30\n",
                        "3/3 [==============================] - 7s 2s/step - loss: 0.3909 - acc: 0.8021 - val_loss: 0.2078 - val_acc: 1.0000\n",
                        "Epoch 14/30\n",
                        "3/3 [==============================] - 7s 2s/step - loss: 0.3414 - acc: 0.8125 - val_loss: 0.3628 - val_acc: 0.8750\n",
                        "Epoch 15/30\n",
                        "3/3 [==============================] - 7s 2s/step - loss: 0.2532 - acc: 0.9271 - val_loss: 0.3940 - val_acc: 0.9583\n",
                        "Epoch 16/30\n",
                        "3/3 [==============================] - 7s 2s/step - loss: 0.3588 - acc: 0.8646 - val_loss: 0.6661 - val_acc: 0.8333\n",
                        "Epoch 17/30\n",
                        "3/3 [==============================] - 7s 2s/step - loss: 0.3459 - acc: 0.8542 - val_loss: 0.6537 - val_acc: 0.8333\n",
                        "Epoch 18/30\n",
                        "3/3 [==============================] - 6s 2s/step - loss: 0.1918 - acc: 0.9375 - val_loss: 0.4310 - val_acc: 0.9583\n",
                        "Epoch 19/30\n",
                        "3/3 [==============================] - 6s 2s/step - loss: 0.2531 - acc: 0.9375 - val_loss: 0.9594 - val_acc: 0.8333\n",
                        "Epoch 20/30\n",
                        "3/3 [==============================] - 6s 2s/step - loss: 0.3269 - acc: 0.8646 - val_loss: 0.3554 - val_acc: 0.9167\n",
                        "Epoch 21/30\n",
                        "3/3 [==============================] - 6s 2s/step - loss: 0.1713 - acc: 0.9375 - val_loss: 0.3326 - val_acc: 0.7917\n",
                        "Epoch 22/30\n",
                        "3/3 [==============================] - 6s 2s/step - loss: 0.4320 - acc: 0.8750 - val_loss: 0.5188 - val_acc: 0.7500\n",
                        "Epoch 23/30\n",
                        "3/3 [==============================] - 6s 2s/step - loss: 0.6408 - acc: 0.8646 - val_loss: 0.4417 - val_acc: 0.7917\n",
                        "Epoch 24/30\n",
                        "3/3 [==============================] - 6s 2s/step - loss: 0.4298 - acc: 0.8125 - val_loss: 0.5866 - val_acc: 0.8333\n",
                        "Epoch 25/30\n",
                        "3/3 [==============================] - 7s 2s/step - loss: 0.3642 - acc: 0.8542 - val_loss: 0.4948 - val_acc: 0.8333\n",
                        "Epoch 26/30\n",
                        "3/3 [==============================] - 7s 2s/step - loss: 0.2601 - acc: 0.8854 - val_loss: 0.5349 - val_acc: 0.8750\n",
                        "Epoch 27/30\n",
                        "3/3 [==============================] - 7s 2s/step - loss: 0.3049 - acc: 0.8229 - val_loss: 0.5665 - val_acc: 0.8750\n",
                        "Epoch 28/30\n",
                        "3/3 [==============================] - 6s 2s/step - loss: 0.2788 - acc: 0.8958 - val_loss: 0.5554 - val_acc: 0.8750\n",
                        "Epoch 29/30\n",
                        "3/3 [==============================] - 7s 2s/step - loss: 0.2975 - acc: 0.9062 - val_loss: 0.2133 - val_acc: 0.9583\n",
                        "Epoch 30/30\n",
                        "3/3 [==============================] - 7s 2s/step - loss: 0.3203 - acc: 0.8854 - val_loss: 0.2642 - val_acc: 0.9583\n"
                    ]
                },
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "<tensorflow.python.keras.callbacks.History at 0x194949e20>"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 10
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "Now, we have trained the model for a bit - let's see how the accuracy actually looks with our previous metrics:"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "source": [
                "class_to_int = test_dataflow.class_indices\n",
                "image_classes = sorted(class_to_int.keys())\n",
                "print(image_classes)\n",
                "gt_classes = test_dataflow.classes\n",
                "print(gt_classes)"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "['brown_spot', 'leaf_blight', 'leaf_smut']\n",
                        "[2, 2, 2, 1, 0, 1, 1, 0, 2, 2, 1, 0, 0, 0, 1, 1, 2, 1, 2, 1, 0, 0, 2, 0]\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "source": [
                "predictions = rice_disease_model.predict(test_dataflow)\n",
                "predicted_classes = list(np.argmax(predictions, axis=1))\n",
                "print(predicted_classes)"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "[2, 2, 2, 1, 0, 1, 1, 0, 2, 2, 1, 0, 0, 0, 2, 1, 2, 1, 2, 1, 0, 0, 2, 0]\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "source": [
                "metrics.ConfusionMatrixDisplay(\n",
                "    metrics.confusion_matrix(gt_classes, predicted_classes), \n",
                "    display_labels=image_classes\n",
                ").plot()"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x1948a97c0>"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 13
                },
                {
                    "output_type": "display_data",
                    "data": {
                        "text/plain": [
                            "<Figure size 432x288 with 2 Axes>"
                        ],
                        "image/png": "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"
                    },
                    "metadata": {
                        "needs_background": "light"
                    }
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "source": [
                "print(metrics.classification_report(gt_classes, predicted_classes, target_names=image_classes))"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "              precision    recall  f1-score   support\n",
                        "\n",
                        "  brown_spot       1.00      1.00      1.00         8\n",
                        " leaf_blight       1.00      0.88      0.93         8\n",
                        "   leaf_smut       0.89      1.00      0.94         8\n",
                        "\n",
                        "    accuracy                           0.96        24\n",
                        "   macro avg       0.96      0.96      0.96        24\n",
                        "weighted avg       0.96      0.96      0.96        24\n",
                        "\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "This is pretty good performance!  We nearly got it perfect.  Parameter tuning might make it work even better (although we would need to do a cross-validation to really be sure)."
            ],
            "metadata": {}
        }
    ],
    "metadata": {
        "orig_nbformat": 4,
        "language_info": {
            "name": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}